from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch

# ≈öcie≈ºki
base_model = "deepseek-ai/deepseek-coder-1.3b-instruct"
adapter_path = "./lora-deepseek-1.3b"

# Wczytaj tokenizer i model z adapterem LoRA
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model, torch_dtype=torch.float16, device_map="auto")
model = PeftModel.from_pretrained(model, adapter_path)

# === PROMPT DO TESTU ===
prompt = """Jeste≈õ koordynatorem dzialu technicznego lokalnego operatora telekomunikacyjnego. Twoim zadaniem jest rozpoznanie przyczyny problemu zgloszonego przez klienta i odpowied≈∫ jednym s≈Çowem, kt√≥re najlepiej opisuje przyczynƒô.
W odpowiedzi podaj tylko jednƒÖ z listy mo≈ºliwych przyczyn.

    "id_zgloszenia": "999999",
    "id_klienta": "999999",
    "temat": "brak us≈Çug",
    "rozmowa": [
      "Klient: klient zg≈Çasza brak mozliwo≈õci korzystania z INT, proszƒô o weryfikacjƒô",
      "Technik: NAT nie by≈Ç w≈ÇƒÖczony na WANie. Us≈Çuga ju≈º dzia≈Ça poprawnie."
    ]

Mo≈ºliwe przyczyny: BD, Zasilacz, T≈Çumienie, Linia ≈õwiat≈Çowodowa, Router, Radi√≥wka, Core, Sprzet, Po stronie klienta, Kabel, SGT, Aktywacja, Konfiguracja, Awaria globalna, EPIX, WiFi, Zapytanie, Sieƒá energetyczna, Zmiana has≈Ça, tSEC, BOK, Dubel, Odwo≈Çanie

Przyczyna (podaj jednƒÖ z listy mo≈ºliwych przyczyn):"""

# Tokenizacja i generowanie
inputs = tokenizer(prompt, return_tensors="pt").to("mps")
outputs = model.generate(**inputs, max_new_tokens=200)
result = tokenizer.decode(outputs[0], skip_special_tokens=True)

# üîç Parsowanie wyniku
print("\n===== WYNIK MODELU =====")
print(result)
